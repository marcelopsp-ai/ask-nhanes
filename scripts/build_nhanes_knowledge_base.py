#!/usr/bin/env python3
"""
NHANES Knowledge Base Builder
Gera documentos a partir de: Wikipedia + Papers + CSV + Conceitos

Uso:
    python3 build_nhanes_knowledge_base.py
"""

import os
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
from pathlib import Path


class NHANESKnowledgeBaseBuilder:
    """Builder para Knowledge Base sobre NHANES e Estat√≠stica"""
    
    def __init__(self, output_dir="data/knowledge_base", csv_path="data/raw/nhanes_2015_2016.csv"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.csv_path = csv_path
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Educational Research Bot)'
        })
    
    # =========================================================================
    # WIKIPEDIA SCRAPING
    # =========================================================================
    
    def scrape_wikipedia(self, topic, output_name):
        """Scrape artigo da Wikipedia"""
        url = f"https://en.wikipedia.org/wiki/{topic}"
        try:
            print(f"  üìÑ Wikipedia: {topic}")
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for tag in soup(['script', 'style', 'sup', 'table', 'img']):
                tag.decompose()
            
            content = soup.find('div', {'id': 'mw-content-text'})
            if not content:
                return False
            
            paragraphs = content.find_all('p')
            text = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
            
            if len(text) > 500:
                output_path = self.output_dir / "wikipedia" / f"{output_name}.txt"
                output_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(f"Source: {url}\n")
                    f.write(f"Topic: {topic.replace('_', ' ')}\n\n")
                    f.write(text[:15000])
                
                print(f"  ‚úÖ Saved: {output_name}.txt ({len(text)} chars)")
                return True
            return False
        except Exception as e:
            print(f"  ‚ùå Error: {e}")
            return False
    
    def scrape_all_wikipedia(self):
        """Scrape todos os artigos relevantes da Wikipedia"""
        print("\nüìö WIKIPEDIA ARTICLES")
        print("=" * 50)
        
        articles = {
            "National_Health_and_Nutrition_Examination_Survey": "nhanes_overview",
            "Body_mass_index": "body_mass_index",
            "Obesity": "obesity",
            "Overweight": "overweight",
            "Linear_regression": "linear_regression",
            "Ordinary_least_squares": "ols_regression",
            "Coefficient_of_determination": "r_squared",
            "Normal_distribution": "normal_distribution",
            "Statistical_hypothesis_testing": "hypothesis_testing",
            "Epidemiology": "epidemiology",
            "Public_health": "public_health",
        }
        
        success = 0
        for topic, name in articles.items():
            if self.scrape_wikipedia(topic, name):
                success += 1
            time.sleep(2)
        
        print(f"\n‚úÖ Wikipedia: {success}/{len(articles)} articles scraped")
        return success
    
    # =========================================================================
    # PAPERS / ACADEMIC SOURCES
    # =========================================================================
    
    def create_paper_summaries(self):
        """Criar resumos de papers acad√™micos sobre NHANES"""
        print("\nüìÑ ACADEMIC PAPERS")
        print("=" * 50)
        
        papers_dir = self.output_dir / "papers"
        papers_dir.mkdir(parents=True, exist_ok=True)
        
        papers = {
            "nhanes_methodology.txt": """# NHANES Methodology and Design

Source: CDC/NCHS Documentation

## Survey Design

The National Health and Nutrition Examination Survey (NHANES) is a program of studies 
designed to assess the health and nutritional status of adults and children in the 
United States.

### Key Features:
- Cross-sectional survey design
- Nationally representative sample
- Combines interviews and physical examinations
- Conducted by NCHS (National Center for Health Statistics)
- Part of CDC (Centers for Disease Control and Prevention)

## Sampling Strategy

NHANES uses a complex, multistage probability sampling design:

1. **Primary Sampling Units (PSUs)**: Counties or groups of counties
2. **Segments**: Census blocks or combinations
3. **Households**: Dwelling units within segments
4. **Individuals**: Persons within households

### Oversampling
Certain subgroups are oversampled to increase precision:
- Hispanic persons
- Non-Hispanic Black persons
- Non-Hispanic Asian persons
- Older adults (60+ years)
- Low-income white persons

## Data Collection

### Interview Component
- Demographic information
- Socioeconomic status
- Dietary intake (24-hour recall)
- Health-related questions

### Examination Component
- Body measurements (height, weight, waist circumference)
- Blood pressure
- Dental examination
- Laboratory tests (blood, urine)

## Survey Weights

NHANES provides sample weights to:
- Account for unequal probability of selection
- Adjust for nonresponse
- Post-stratify to population totals

**Important**: Always use sample weights for nationally representative estimates!
""",

            "obesity_prevalence_usa.txt": """# Obesity Prevalence in the United States

Source: CDC NCHS Data Briefs + Academic Literature

## Current Statistics (NHANES Data)

### Adult Obesity (Age 20+)
- Overall prevalence: ~42.4%
- Severe obesity (BMI ‚â•40): ~9.2%
- Trend: Increasing since 1999-2000

### By Sex
- Men: ~43.0%
- Women: ~41.9%

### By Age Group
- 20-39 years: ~40.0%
- 40-59 years: ~44.8% (highest)
- 60+ years: ~42.8%

### By Race/Ethnicity
- Non-Hispanic Black: ~49.6%
- Hispanic: ~44.8%
- Non-Hispanic White: ~42.2%
- Non-Hispanic Asian: ~17.4%

## BMI Classification (WHO)

| Category | BMI Range |
|----------|-----------|
| Underweight | < 18.5 |
| Normal | 18.5 - 24.9 |
| Overweight | 25.0 - 29.9 |
| Obesity Class I | 30.0 - 34.9 |
| Obesity Class II | 35.0 - 39.9 |
| Obesity Class III | ‚â• 40.0 |

## Health Consequences

Obesity increases risk for:
- Type 2 diabetes
- Cardiovascular disease
- Hypertension
- Certain cancers
- Sleep apnea
- Osteoarthritis
""",

            "regression_health_studies.txt": """# Regression Analysis in Health Studies

Source: Biostatistics and Epidemiology Literature

## Why Regression in Health Research?

Regression analysis allows researchers to:
- Identify risk factors for diseases
- Quantify relationships between variables
- Control for confounding variables
- Make predictions about health outcomes

## Linear Regression in NHANES

### Common Applications

1. **BMI Prediction**
   - Predictors: Age, sex, diet, physical activity
   - Model: BMI = Œ≤‚ÇÄ + Œ≤‚ÇÅ(Age) + Œ≤‚ÇÇ(Sex) + ...

2. **Blood Pressure Studies**
   - Predictors: BMI, sodium intake, age, smoking
   - Outcome: Systolic/Diastolic BP

### Key Assumptions

1. **Linearity**: Relationship between X and Y is linear
2. **Independence**: Observations are independent
3. **Normality**: Residuals are normally distributed
4. **Homoscedasticity**: Constant variance of residuals

### Interpreting Results

- R¬≤ = Proportion of variance explained
- Œ≤ = Change in Y for 1-unit change in X
- p < 0.05 = Statistically significant
""",

            "statistical_tests_health.txt": """# Statistical Tests in Health Research

Source: Biostatistics Textbooks

## Choosing the Right Test

### For Comparing Groups

| Comparison | Parametric | Non-Parametric |
|------------|------------|----------------|
| 2 groups (independent) | t-test | Mann-Whitney U |
| 2 groups (paired) | Paired t-test | Wilcoxon signed-rank |
| 3+ groups | ANOVA | Kruskal-Wallis |

### For Relationships

| Variables | Test |
|-----------|------|
| 2 continuous | Pearson correlation |
| 2 continuous (non-normal) | Spearman correlation |
| Continuous + Categorical | t-test / ANOVA |
| 2 categorical | Chi-square |

## Tests Used in NHANES Analysis

### 1. Shapiro-Wilk (Normality)
- H‚ÇÄ: Data is normally distributed
- p > 0.05 ‚Üí Assume normality

### 2. Independent t-test
- Compares means of 2 groups
- Example: Weight by Sex

### 3. Mann-Whitney U
- Non-parametric alternative to t-test
- Uses ranks instead of raw values

### 4. ANOVA
- Compares means of 3+ groups
- F-statistic = Between/Within variance

### 5. Pearson Correlation
- Measures linear relationship
- r = -1 to +1

### 6. Breusch-Pagan
- Tests homoscedasticity
- p > 0.05 = Constant variance
"""
        }
        
        for filename, content in papers.items():
            filepath = papers_dir / filename
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content.strip())
            print(f"  ‚úÖ Created: {filename}")
        
        print(f"\n‚úÖ Papers: {len(papers)} documents created")
        return len(papers)
    
    # =========================================================================
    # CONCEITOS ESTAT√çSTICOS
    # =========================================================================
    
    def create_concept_docs(self):
        """Criar documentos sobre conceitos estat√≠sticos"""
        print("\nüìù STATISTICAL CONCEPTS")
        print("=" * 50)
        
        concepts_dir = self.output_dir / "conceitos"
        concepts_dir.mkdir(parents=True, exist_ok=True)
        
        concepts = {
            "medidas_tendencia_central.txt": """# Medidas de Tend√™ncia Central

Source: Estat√≠stica Descritiva

## Defini√ß√£o
Medidas que indicam o "centro" ou valor t√≠pico de um conjunto de dados.

## Tipos de M√©dia

### 1. M√©dia Aritm√©tica
- Soma de todos os valores dividida pela quantidade
- F√≥rmula: xÃÑ = Œ£x·µ¢ / n
- Sens√≠vel a outliers

### 2. M√©dia Geom√©trica
- Raiz n-√©sima do produto de n valores
- Sempre menor que aritm√©tica
- Uso: Taxas de crescimento

### 3. M√©dia Harm√¥nica
- Inverso da m√©dia dos inversos
- Sempre a menor das tr√™s m√©dias
- Uso: Velocidades, taxas

## Mediana
- Valor central quando dados ordenados
- N√£o afetada por outliers

## Moda
- Valor mais frequente
- Pode n√£o existir ou haver m√∫ltiplas

## Compara√ß√£o M√©dia vs Mediana

| Distribui√ß√£o | Rela√ß√£o |
|--------------|---------|
| Sim√©trica | M√©dia ‚âà Mediana |
| Assim√©trica direita | M√©dia > Mediana |
| Assim√©trica esquerda | M√©dia < Mediana |
""",

            "medidas_dispersao.txt": """# Medidas de Dispers√£o

Source: Estat√≠stica Descritiva

## Defini√ß√£o
Medidas que indicam o quanto os dados est√£o "espalhados".

## Principais Medidas

### 1. Amplitude
- Diferen√ßa entre m√°ximo e m√≠nimo
- Muito sens√≠vel a outliers

### 2. Vari√¢ncia (œÉ¬≤)
- M√©dia dos desvios quadr√°ticos
- Amostral: s¬≤ = Œ£(x·µ¢ - xÃÑ)¬≤ / (n-1)

### 3. Desvio Padr√£o (œÉ ou s)
- Raiz quadrada da vari√¢ncia
- Mesma unidade dos dados
- ~68% dentro de ¬±1œÉ (normal)
- ~95% dentro de ¬±2œÉ (normal)

### 4. Coeficiente de Varia√ß√£o (CV)
- CV = (s / xÃÑ) √ó 100%
- CV < 15%: Baixa variabilidade
- CV > 30%: Alta variabilidade

### 5. Intervalo Interquartil (IQR)
- IQR = Q3 - Q1
- Cont√©m 50% dos dados centrais
- Robusto a outliers

## Identifica√ß√£o de Outliers

Regra do IQR:
- Limite inferior: Q1 - 1.5 √ó IQR
- Limite superior: Q3 + 1.5 √ó IQR
""",

            "pressupostos_regressao.txt": """# Pressupostos da Regress√£o Linear

Source: Econometria e Bioestat√≠stica

## Os 5 Pressupostos

### 1. Linearidade
- Rela√ß√£o entre X e Y √© linear
- Verificar: Gr√°fico de dispers√£o

### 2. Independ√™ncia
- Observa√ß√µes s√£o independentes
- Verificar: Durbin-Watson (ideal ‚âà 2.0)

### 3. Normalidade
- Res√≠duos seguem distribui√ß√£o normal
- Verificar: Shapiro-Wilk, Q-Q Plot

### 4. Homoscedasticidade
- Vari√¢ncia constante dos res√≠duos
- Verificar: Breusch-Pagan

### 5. Aus√™ncia de Multicolinearidade
- Vari√°veis independentes n√£o correlacionadas
- Verificar: VIF < 5

## Testes Diagn√≥sticos

| Pressuposto | Teste | Bom resultado |
|-------------|-------|---------------|
| Independ√™ncia | Durbin-Watson | DW ‚âà 2.0 |
| Normalidade | Shapiro-Wilk | p > 0.05 |
| Homoscedasticidade | Breusch-Pagan | p > 0.05 |
| Multicolinearidade | VIF | VIF < 5 |
""",

            "interpretacao_regressao.txt": """# Interpreta√ß√£o de Resultados de Regress√£o

Source: An√°lise de Dados

## Bloco 1: Performance Geral

### R¬≤ (R-squared)
- Propor√ß√£o da vari√¢ncia explicada
- 0 a 1 (0% a 100%)

### R¬≤ Ajustado
- Penaliza adi√ß√£o de vari√°veis
- Se R¬≤ ‚âà R¬≤ ajustado: OK

### F-statistic
- Testa utilidade do modelo
- p < 0.05: Modelo √© √∫til

## Bloco 2: Coeficientes

### Intercepto (Œ≤‚ÇÄ)
- Valor de Y quando todos X = 0

### Coeficientes (Œ≤·µ¢)
- Mudan√ßa em Y para cada 1 unidade em X·µ¢

### P-valor
- p < 0.05: Significativo
- p < 0.001: Altamente significativo

## Bloco 3: Diagn√≥stico

### Durbin-Watson
- Ideal: ‚âà 2.0
- < 1.5 ou > 2.5: Problema

### Omnibus / Jarque-Bera
- p > 0.05: Res√≠duos normais

### Skew / Kurtosis
- Skew = 0: Sim√©trico
- Kurtosis = 3: Normal
"""
        }
        
        for filename, content in concepts.items():
            filepath = concepts_dir / filename
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content.strip())
            print(f"  ‚úÖ Created: {filename}")
        
        print(f"\n‚úÖ Concepts: {len(concepts)} documents created")
        return len(concepts)
    
    # =========================================================================
    # ESTAT√çSTICAS DO CSV
    # =========================================================================
    
    def generate_csv_stats(self):
        """Gerar documentos de estat√≠sticas a partir do CSV NHANES"""
        print("\nüìä CSV STATISTICS")
        print("=" * 50)
        
        if not os.path.exists(self.csv_path):
            print(f"  ‚ö†Ô∏è  CSV n√£o encontrado: {self.csv_path}")
            print("  ‚ÑπÔ∏è  Coloque o arquivo nhanes_2015_2016.csv em data/raw/")
            return 0
        
        stats_dir = self.output_dir / "estatisticas"
        stats_dir.mkdir(parents=True, exist_ok=True)
        
        df = pd.read_csv(self.csv_path)
        df = df[df['Idade'] >= 18].copy()
        
        print(f"  üìÇ Loaded: {len(df):,} records")
        
        docs_created = 0
        
        # 1. Resumo Geral
        resumo = f"""# NHANES 2015-2016 - Resumo do Dataset

Source: data/raw/nhanes_2015_2016.csv

## Informa√ß√µes Gerais
- Total de registros (adultos): {len(df):,}
- Vari√°veis dispon√≠veis: {len(df.columns)}

## Estat√≠sticas Descritivas

### IMC (√çndice de Massa Corporal)
- N: {df['IMC'].notna().sum():,}
- M√©dia: {df['IMC'].mean():.2f}
- Mediana: {df['IMC'].median():.2f}
- Desvio Padr√£o: {df['IMC'].std():.2f}
- M√≠nimo: {df['IMC'].min():.2f}
- M√°ximo: {df['IMC'].max():.2f}

### Peso (kg)
- M√©dia: {df['Peso_kg'].mean():.2f}
- Mediana: {df['Peso_kg'].median():.2f}
- Desvio Padr√£o: {df['Peso_kg'].std():.2f}

### Altura (cm)
- M√©dia: {df['Altura_cm'].mean():.2f}
- Mediana: {df['Altura_cm'].median():.2f}
- Desvio Padr√£o: {df['Altura_cm'].std():.2f}
"""
        
        with open(stats_dir / "resumo_geral.txt", 'w', encoding='utf-8') as f:
            f.write(resumo)
        print("  ‚úÖ Created: resumo_geral.txt")
        docs_created += 1
        
        # 2. IMC por Faixa Et√°ria
        df['FaixaEtaria'] = pd.cut(df['Idade'], 
                                    bins=[18, 29, 44, 59, 120],
                                    labels=['18-29', '30-44', '45-59', '60+'])
        
        imc_idade = df.groupby('FaixaEtaria')['IMC'].agg(['count', 'mean', 'median', 'std']).round(2)
        
        imc_doc = f"""# IMC por Faixa Et√°ria - NHANES 2015-2016

Source: data/raw/nhanes_2015_2016.csv

## Estat√≠sticas por Grupo

| Faixa Et√°ria | N | M√©dia | Mediana | DP |
|--------------|---|-------|---------|-----|
"""
        for idx, row in imc_idade.iterrows():
            imc_doc += f"| {idx} | {int(row['count']):,} | {row['mean']:.2f} | {row['median']:.2f} | {row['std']:.2f} |\n"
        
        imc_doc += f"""
## Interpreta√ß√£o

A faixa et√°ria **45-59 anos** apresenta o maior IMC m√©dio, 
classificado como **obesidade** segundo a OMS (IMC ‚â• 30).

### Classifica√ß√£o OMS do IMC
- < 18.5: Baixo peso
- 18.5 - 24.9: Normal
- 25.0 - 29.9: Sobrepeso
- ‚â• 30.0: Obesidade
"""
        
        with open(stats_dir / "imc_por_idade.txt", 'w', encoding='utf-8') as f:
            f.write(imc_doc)
        print("  ‚úÖ Created: imc_por_idade.txt")
        docs_created += 1
        
        # 3. Peso por Sexo
        sexo_map = {1: 'Masculino', 2: 'Feminino'}
        df['SexoNome'] = df['Sexo'].map(sexo_map)
        peso_sexo = df.groupby('SexoNome')['Peso_kg'].agg(['count', 'mean', 'median', 'std']).round(2)
        
        peso_doc = f"""# Peso por Sexo - NHANES 2015-2016

Source: data/raw/nhanes_2015_2016.csv

## Estat√≠sticas por Sexo

| Sexo | N | M√©dia (kg) | Mediana (kg) | DP |
|------|---|------------|--------------|-----|
"""
        for idx, row in peso_sexo.iterrows():
            peso_doc += f"| {idx} | {int(row['count']):,} | {row['mean']:.2f} | {row['median']:.2f} | {row['std']:.2f} |\n"
        
        with open(stats_dir / "peso_por_sexo.txt", 'w', encoding='utf-8') as f:
            f.write(peso_doc)
        print("  ‚úÖ Created: peso_por_sexo.txt")
        docs_created += 1
        
        # 4. Correla√ß√µes
        corr_vars = ['Idade', 'Altura_cm', 'Peso_kg', 'IMC']
        corr_matrix = df[corr_vars].corr().round(3)
        
        corr_doc = f"""# Matriz de Correla√ß√£o - NHANES 2015-2016

Source: data/raw/nhanes_2015_2016.csv

## Correla√ß√µes de Pearson

|  | Idade | Altura | Peso | IMC |
|--|-------|--------|------|-----|
| Idade | 1.000 | {corr_matrix.loc['Idade', 'Altura_cm']:.3f} | {corr_matrix.loc['Idade', 'Peso_kg']:.3f} | {corr_matrix.loc['Idade', 'IMC']:.3f} |
| Altura | {corr_matrix.loc['Altura_cm', 'Idade']:.3f} | 1.000 | {corr_matrix.loc['Altura_cm', 'Peso_kg']:.3f} | {corr_matrix.loc['Altura_cm', 'IMC']:.3f} |
| Peso | {corr_matrix.loc['Peso_kg', 'Idade']:.3f} | {corr_matrix.loc['Peso_kg', 'Altura_cm']:.3f} | 1.000 | {corr_matrix.loc['Peso_kg', 'IMC']:.3f} |
| IMC | {corr_matrix.loc['IMC', 'Idade']:.3f} | {corr_matrix.loc['IMC', 'Altura_cm']:.3f} | {corr_matrix.loc['IMC', 'Peso_kg']:.3f} | 1.000 |

## Interpreta√ß√£o

### Correla√ß√µes Fortes (|r| > 0.5)
- **Peso √ó IMC**: Muito forte (esperado)
- **Altura √ó Peso**: Moderada a forte

### Classifica√ß√£o de Cohen
- |r| < 0.1: Desprez√≠vel
- |r| 0.1 - 0.3: Fraca
- |r| 0.3 - 0.5: Moderada
- |r| > 0.5: Forte
"""
        
        with open(stats_dir / "correlacoes.txt", 'w', encoding='utf-8') as f:
            f.write(corr_doc)
        print("  ‚úÖ Created: correlacoes.txt")
        docs_created += 1
        
        print(f"\n‚úÖ CSV Stats: {docs_created} documents created")
        return docs_created
    
    # =========================================================================
    # RUN ALL
    # =========================================================================
    
    def run_all(self):
        """Executar todo o build da Knowledge Base"""
        print("\n" + "=" * 60)
        print("üöÄ BUILDING NHANES KNOWLEDGE BASE")
        print("=" * 60)
        
        total = 0
        
        total += self.scrape_all_wikipedia()
        total += self.create_paper_summaries()
        total += self.create_concept_docs()
        total += self.generate_csv_stats()
        
        print("\n" + "=" * 60)
        print("‚úÖ KNOWLEDGE BASE BUILD COMPLETE!")
        print("=" * 60)
        
        total_files = sum(1 for _ in self.output_dir.rglob('*.txt'))
        
        print(f"üìä Total documents: {total_files}")
        print(f"üìÅ Location: {self.output_dir}")
        
        print("\nüìÇ Breakdown:")
        for subdir in self.output_dir.iterdir():
            if subdir.is_dir():
                count = sum(1 for _ in subdir.rglob('*.txt'))
                if count > 0:
                    print(f"   {subdir.name}/: {count} files")
        
        print("=" * 60)
        return total_files


if __name__ == "__main__":
    builder = NHANESKnowledgeBaseBuilder()
    builder.run_all()